from typing import List, Dict, Optional, Tuple
from sentence_transformers import SentenceTransformer
from text_processing import extract_keywords, extract_candidate_mentions
from database import Database
from config import KEYWORD_THRESHOLD, RAG_THRESHOLD, MAX_SEARCH_RESULTS

class SearchEngine:
    def __init__(self):
        self.db = Database()
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def search(self, query: str) -> Tuple[List[Dict], str]:
        """
        Recherche hybride : mots-clÃ©s d'abord, puis RAG si nÃ©cessaire
        Retourne (rÃ©sultats, mÃ©thode_utilisÃ©e)
        """
        # 1. Extraire les mots-clÃ©s et candidats mentionnÃ©s
        keywords = extract_keywords(query)
        candidate = extract_candidate_mentions(query)
        
        print(f"Recherche pour: '{query}'")
        print(f"Mots-clÃ©s extraits: {keywords}")
        print(f"Candidat dÃ©tectÃ©: {candidate}")
        
        # 2. PremiÃ¨re Ã©tape : recherche par mots-clÃ©s
        keyword_results = self._search_by_keywords(keywords, candidate)
        
        if self._is_sufficient_results(keyword_results):
            print(f"Recherche par mots-clÃ©s suffisante: {len(keyword_results)} rÃ©sultats")
            return self._rank_results(keyword_results, query), "keywords"
        
        # 3. DeuxiÃ¨me Ã©tape : recherche RAG
        print("Recherche par mots-clÃ©s insuffisante, passage au RAG")
        rag_results = self._search_by_rag(query, candidate)
        
        if rag_results:
            print(f"Recherche RAG: {len(rag_results)} rÃ©sultats")
            return rag_results, "rag"
        
        # 4. Fallback : retourner les rÃ©sultats des mots-clÃ©s mÃªme s'ils sont peu nombreux
        print("Aucun rÃ©sultat RAG, retour aux mots-clÃ©s")
        return self._rank_results(keyword_results, query), "keywords_fallback"
    
    def _search_by_keywords(self, keywords: List[str], candidate: Optional[str] = None) -> List[Dict]:
        """Recherche par mots-clÃ©s"""
        if not keywords:
            return []
        
        results = self.db.search_by_keywords(keywords, candidate)
        
        # Calculer un score de pertinence basÃ© sur le nombre de mots-clÃ©s matchÃ©s
        for result in results:
            matched_keywords = set(keywords) & set(result.get('keywords', []))
            result['keyword_score'] = len(matched_keywords) / len(keywords)
        
        return results
    
    def _search_by_rag(self, query: str, candidate: Optional[str] = None) -> List[Dict]:
        """Recherche par similaritÃ© vectorielle (RAG)"""
        # GÃ©nÃ©rer l'embedding de la question
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # Recherche vectorielle
        results = self.db.search_by_similarity(
            query_embedding, 
            candidate, 
            limit=MAX_SEARCH_RESULTS,
            threshold=RAG_THRESHOLD
        )
        
        return results
    
    def _is_sufficient_results(self, results: List[Dict], min_score: float = KEYWORD_THRESHOLD) -> bool:
        """DÃ©termine si les rÃ©sultats de mots-clÃ©s sont suffisants"""
        if not results:
            return False
        
        # VÃ©rifier qu'au moins un rÃ©sultat a un bon score
        good_results = [r for r in results if r.get('keyword_score', 0) >= min_score]
        return len(good_results) >= 1
    
    def _rank_results(self, results: List[Dict], query: str) -> List[Dict]:
        """Classe les rÃ©sultats par pertinence"""
        if not results:
            return []
        
        # Trier par score de mots-clÃ©s dÃ©croissant
        sorted_results = sorted(
            results, 
            key=lambda x: x.get('keyword_score', 0), 
            reverse=True
        )
        
        return sorted_results[:MAX_SEARCH_RESULTS]
    
    def get_context_for_llm(self, search_results: List[Dict], max_tokens: int = 3000) -> str:
        """
        PrÃ©pare le contexte pour envoyer au LLM
        Estime grossiÃ¨rement les tokens (1 token â‰ˆ 4 caractÃ¨res)
        """
        if not search_results:
            return ""
        
        context_parts = []
        current_length = 0
        max_chars = max_tokens * 4
        
        for i, result in enumerate(search_results):
            # Formater le rÃ©sultat
            part = f"""
Document {i+1}:
Source: {result.get('source_link', 'N/A')}
Candidat: {result.get('candidate', 'Information gÃ©nÃ©rale')}
Section: {result.get('section', 'N/A')}
Contenu: {result.get('text', '')}
---
"""
            
            # VÃ©rifier si on dÃ©passe la limite
            if current_length + len(part) > max_chars:
                break
            
            context_parts.append(part)
            current_length += len(part)
        
        return '\n'.join(context_parts)
    
    def format_sources(self, search_results: List[Dict]) -> str:
        """Formate les sources pour l'affichage"""
        if not search_results:
            return ""
        
        sources = []
        seen_sources = set()
        
        for result in search_results:
            source_link = result.get('source_link')
            candidate = result.get('candidate', 'Information gÃ©nÃ©rale')
            
            if source_link and source_link not in seen_sources:
                sources.append(f"â€¢ [{candidate}] {source_link}")
                seen_sources.add(source_link)
        
        if sources:
            return "\n\nðŸ“š **Sources:**\n" + '\n'.join(sources)
        return ""